{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 924 documents in the index 'technical_ind'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [00:08<00:00, 111.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 924 documents in the index 'objective_ind'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [00:08<00:00, 107.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "index_names = ['technical_ind', 'objective_ind']\n",
    "corpuses = {'technical_ind':{}, 'objective_ind':{}}\n",
    "for index_name in index_names:\n",
    "    ndocs = int(client.cat.count(index=index_name, format = \"json\")[0]['count'])\n",
    "    print(f\"There are {ndocs} documents in the index '{index_name}'\")\n",
    "\n",
    "\n",
    "    corpus = corpuses[index_name]    # will store _normalized_ tfidf for each document, key is internal elasticsearch id, value is dictionary of term -> tf-idf weight\n",
    "    for s in tqdm.tqdm(scan(client, index=index_name, query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "        terms = []\n",
    "        freqs = []\n",
    "        dfs = []\n",
    "\n",
    "        tv = client.termvectors(index=index_name, id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "        if 'text' in tv['term_vectors']:   # just in case some document has no field named 'text'\n",
    "            for t in tv['term_vectors']['text']['terms']:\n",
    "                f = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "\n",
    "                terms.append(t)\n",
    "                freqs.append(tv['term_vectors']['text']['terms'][t]['term_freq'])\n",
    "                dfs.append(tv['term_vectors']['text']['terms'][t]['doc_freq'])\n",
    "\n",
    "        # vector computations for tf-idf; l2-normalized for further calculations..\n",
    "        tfidf = np.array(freqs) * np.log2(ndocs / np.array(dfs))\n",
    "        tfidf /= np.linalg.norm(tfidf)\n",
    "\n",
    "        # save in corpus dictionary\n",
    "        corpus[s['_id']] = {t: tfidf[j] for j, t in enumerate(terms)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "print(corpuses['technical_ind'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports ###\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import heapq\n",
    "import tqdm\n",
    "import uuid\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from elasticsearch_dsl import Search, Index, analyzer, tokenizer\n",
    "from elasticsearch_dsl.query import Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(query: str) -> str:\n",
    "    res = ind.analyze(body={'analyzer':'default', 'text': query})\n",
    "    query_stemmed = ''\n",
    "    first = True\n",
    "    for r in res['tokens']:\n",
    "        if not first:\n",
    "            query_stemmed += ' ' + r['token']\n",
    "        else:\n",
    "            query_stemmed += r['token']\n",
    "            first = False\n",
    "    return query_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(d: list[tuple[str, float]]) -> float:\n",
    "    return np.sqrt(sum([freq*freq for term, freq in d]))\n",
    "\n",
    "\n",
    "def normalize(d1: list[tuple[str, float]]):\n",
    "    normm = norm(d1)\n",
    "    return [(k, v/normm) for k, v in d1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/924 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Objectives_files/00a24006-cb98-4dea-8750-52dcd93aac92.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(scan(client, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective_ind\u001b[39m\u001b[38;5;124m'\u001b[39m, query\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_all\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}}}), total\u001b[38;5;241m=\u001b[39mndocs):\n\u001b[0;32m     20\u001b[0m     docid \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;66;03m# use path as id\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mcorpuses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobjective_ind\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocid\u001b[49m\u001b[43m]\u001b[49m   \u001b[38;5;66;03m# gets weights as a python dict of term -> weight (see remark above)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     sims[docid] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m query\u001b[38;5;241m.\u001b[39msplit():  \u001b[38;5;66;03m# gets terms as a list\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Objectives_files/00a24006-cb98-4dea-8750-52dcd93aac92.txt'"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "r = 10  # only return r top docs\n",
    "query = 'win'\n",
    "sims = dict()\n",
    "\n",
    "l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# get nr. of docs; just for the progress bar\n",
    "ndocs = int(client.cat.count(index='objective_ind', format = \"json\")[0]['count'])\n",
    "\n",
    "# scan through docs, compute cosine sim between query and each doc\n",
    "for s in tqdm.tqdm(scan(client, index='objective_ind', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "    docid = s['_source']['path']   # use path as id\n",
    "    weights = corpuses['objective_ind'][docid]   # gets weights as a python dict of term -> weight (see remark above)\n",
    "    sims[docid] = 0.0\n",
    "    for w in query.split():  # gets terms as a list\n",
    "        if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "            sims[docid] += weights[w]   # accumulates if w in current doc\n",
    "    # normalize sim\n",
    "    sims[docid] /= l2query\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "pprint(sorted_answer[:r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
